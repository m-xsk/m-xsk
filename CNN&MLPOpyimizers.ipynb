{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5zRaXnWUCDK9Y0F7E/ng7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-xsk/m-xsk/blob/main/CNN%26MLPOpyimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTCpvSksd_33"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow tensorflow-datasets matplotlib pandas\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import layers, models, optimizers, losses\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the rock_paper_scissors dataset\n",
        "dataset, info = tfds.load('rock_paper_scissors', with_info=True, as_supervised=True)\n",
        "train_ds_raw, test_ds_raw = dataset['train'], dataset['test']\n",
        "\n",
        "# Resize dimension\n",
        "IMG_SIZE = 100\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "# TensorFlow data pipeline for CNN training\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Split the train_ds into train and validation sets (90% train, 10% validation)\n",
        "train_size = int(0.9 * len(train_ds_raw))\n",
        "val_size = len(train_ds_raw) - train_size\n",
        "\n",
        "# Shuffle and split the train_ds into train_ds and val_ds\n",
        "train_ds = (\n",
        "    train_ds_raw\n",
        "    .take(train_size)  # 90% of the dataset for training\n",
        "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .cache()\n",
        "    .shuffle(1000)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    train_ds_raw\n",
        "    .skip(train_size)  # 10% for validation\n",
        "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Prepare data as NumPy arrays for MLP models\n",
        "images_all = []\n",
        "labels_all = []\n",
        "\n",
        "for image, label in tfds.as_numpy(train_ds_raw.map(preprocess)):\n",
        "    images_all.append(image)\n",
        "    labels_all.append(label)\n",
        "\n",
        "images_all = np.array(images_all)\n",
        "labels_all = np.array(labels_all)\n",
        "\n",
        "# Split into training and validation sets (90% train, 10% val)\n",
        "images_train, images_val, labels_train, labels_val = train_test_split(\n",
        "    images_all, labels_all, test_size=0.1, random_state=42, stratify=labels_all\n",
        ")\n",
        "\n",
        "# Prepare test_ds (no changes needed)\n",
        "test_ds = (\n",
        "    test_ds_raw\n",
        "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "jisEd5e5jF84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp_model(optimizer, input_shape=(100, 100, 3), num_classes=3):\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "3PS8seRykVV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizers_dict = {\n",
        "    'SGD': optimizers.SGD(),\n",
        "    'SGD_Momentum': optimizers.SGD(momentum=0.9),\n",
        "    'Adagrad': optimizers.Adagrad(),\n",
        "    'RMSProp': optimizers.RMSprop(),\n",
        "    'Adam': optimizers.Adam()\n",
        "}\n"
      ],
      "metadata": {
        "id": "-loYkhlTkk7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_histories = {}\n",
        "mlp_models = {}\n"
      ],
      "metadata": {
        "id": "jrZdTKXQko9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store history of each optimizer's training\n",
        "history_mlp = {}\n",
        "best_model_mlp = None\n",
        "best_optimizer_mlp = \"\"\n",
        "best_acc_mlp = 0\n",
        "\n",
        "# Iterate through each optimizer in the optimizer list\n",
        "for optimizer_name, optimizer in optimizers_dict.items():\n",
        "    print(f\"Training with {optimizer_name}\")\n",
        "\n",
        "    # Create and compile the MLP model using the current optimizer\n",
        "    model_mlp = create_mlp_model(optimizer)  # Assuming model creation function already uses Input()\n",
        "\n",
        "    # Train the model using train and validation data from NumPy arrays\n",
        "    hist_mlp = model_mlp.fit(\n",
        "        images_train, labels_train,\n",
        "        validation_data=(images_val, labels_val),\n",
        "        epochs=5, verbose=1\n",
        "    )\n",
        "\n",
        "    # Store the training history for each optimizer\n",
        "    history_mlp[optimizer_name] = hist_mlp.history\n",
        "\n",
        "    # Get the validation accuracy of the current model\n",
        "    current_val_acc_mlp = max(hist_mlp.history['val_acc'])\n",
        "\n",
        "    # Update the best model if the current model's accuracy is higher\n",
        "    if best_model_mlp is None or current_val_acc_mlp > best_acc_mlp:\n",
        "        best_model_mlp = model_mlp\n",
        "        best_acc_mlp = current_val_acc_mlp\n",
        "        best_optimizer_mlp = optimizer_name\n",
        "\n",
        "# Output the best optimizer and its validation accuracy\n",
        "print(f\"Best MLP optimizer: {best_optimizer_mlp} with validation accuracy: {best_acc_mlp:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjZGlr5dlZZ-",
        "outputId": "e5951520-5240-4126-c648-a5aebafaf2d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SGD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - acc: 0.3352 - loss: 3.2193 - val_acc: 0.3333 - val_loss: 1.1000\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - acc: 0.3460 - loss: 1.1025 - val_acc: 0.4405 - val_loss: 1.0917\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - acc: 0.3813 - loss: 1.0887 - val_acc: 0.3333 - val_loss: 1.0989\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 90ms/step - acc: 0.3327 - loss: 1.0982 - val_acc: 0.3333 - val_loss: 1.0988\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 104ms/step - acc: 0.3373 - loss: 1.0991 - val_acc: 0.3333 - val_loss: 1.0987\n",
            "Training with SGD_Momentum\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 242ms/step - acc: 0.3482 - loss: 4.1561 - val_acc: 0.3333 - val_loss: 1.0991\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - acc: 0.3278 - loss: 1.1022 - val_acc: 0.3333 - val_loss: 1.0987\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 148ms/step - acc: 0.3384 - loss: 1.1001 - val_acc: 0.3333 - val_loss: 1.0987\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 148ms/step - acc: 0.3257 - loss: 1.0998 - val_acc: 0.3333 - val_loss: 1.0988\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 147ms/step - acc: 0.3543 - loss: 1.0991 - val_acc: 0.3333 - val_loss: 1.0987\n",
            "Training with Adagrad\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 212ms/step - acc: 0.3554 - loss: 1.5550 - val_acc: 0.3929 - val_loss: 1.0430\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 220ms/step - acc: 0.4658 - loss: 1.0503 - val_acc: 0.5476 - val_loss: 0.9892\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 206ms/step - acc: 0.5137 - loss: 0.9839 - val_acc: 0.4087 - val_loss: 0.9577\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 208ms/step - acc: 0.5323 - loss: 0.9489 - val_acc: 0.8254 - val_loss: 0.7488\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 214ms/step - acc: 0.6003 - loss: 0.8552 - val_acc: 0.8413 - val_loss: 0.6771\n",
            "Training with RMSProp\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 253ms/step - acc: 0.3208 - loss: 39.3757 - val_acc: 0.3333 - val_loss: 1.0986\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 247ms/step - acc: 0.3339 - loss: 1.1042 - val_acc: 0.3254 - val_loss: 1.0986\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 261ms/step - acc: 0.3126 - loss: 1.0989 - val_acc: 0.3333 - val_loss: 1.0986\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 242ms/step - acc: 0.3448 - loss: 1.0992 - val_acc: 0.3333 - val_loss: 1.0986\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 259ms/step - acc: 0.3232 - loss: 1.0988 - val_acc: 0.3333 - val_loss: 1.0986\n",
            "Training with Adam\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 280ms/step - acc: 0.3406 - loss: 21.6032 - val_acc: 0.3571 - val_loss: 1.0882\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 265ms/step - acc: 0.4001 - loss: 1.0634 - val_acc: 0.4246 - val_loss: 1.0125\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 281ms/step - acc: 0.4290 - loss: 1.0358 - val_acc: 0.3929 - val_loss: 1.0610\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 265ms/step - acc: 0.3761 - loss: 1.0724 - val_acc: 0.4206 - val_loss: 1.0325\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 278ms/step - acc: 0.4142 - loss: 1.0314 - val_acc: 0.3651 - val_loss: 1.0868\n",
            "Best MLP optimizer: Adagrad with validation accuracy: 0.8413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training the MLP models, print the training and validation history for each epoch\n",
        "for optimizer_name, hist in history_mlp.items():\n",
        "    print(f\"\\n--- Training History for {optimizer_name} ---\")\n",
        "\n",
        "    # Iterate through each epoch and print metrics\n",
        "    for epoch in range(len(hist['loss'])):\n",
        "        print(f\"Epoch {epoch + 1}:\")\n",
        "        print(f\"  Training Loss: {hist['loss'][epoch]:.4f}\")\n",
        "        print(f\"  Training Accuracy: {hist['acc'][epoch]:.4f}\")\n",
        "        print(f\"  Validation Loss: {hist['val_loss'][epoch]:.4f}\")\n",
        "        print(f\"  Validation Accuracy: {hist['val_acc'][epoch]:.4f}\")\n",
        "        print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4djtoPL3pP83",
        "outputId": "e126898e-c95c-4d4e-81ab-d9a3c6cc742b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training History for SGD ---\n",
            "Epoch 1:\n",
            "  Training Loss: 1.7542\n",
            "  Training Accuracy: 0.3364\n",
            "  Validation Loss: 1.1000\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 2:\n",
            "  Training Loss: 1.0969\n",
            "  Training Accuracy: 0.3576\n",
            "  Validation Loss: 1.0917\n",
            "  Validation Accuracy: 0.4405\n",
            "----------------------------------------\n",
            "Epoch 3:\n",
            "  Training Loss: 1.0868\n",
            "  Training Accuracy: 0.3814\n",
            "  Validation Loss: 1.0989\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 4:\n",
            "  Training Loss: 1.0962\n",
            "  Training Accuracy: 0.3439\n",
            "  Validation Loss: 1.0988\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 5:\n",
            "  Training Loss: 1.0990\n",
            "  Training Accuracy: 0.3316\n",
            "  Validation Loss: 1.0987\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "\n",
            "--- Training History for SGD_Momentum ---\n",
            "Epoch 1:\n",
            "  Training Loss: 2.2864\n",
            "  Training Accuracy: 0.3289\n",
            "  Validation Loss: 1.0991\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 2:\n",
            "  Training Loss: 1.1016\n",
            "  Training Accuracy: 0.3241\n",
            "  Validation Loss: 1.0987\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 3:\n",
            "  Training Loss: 1.0995\n",
            "  Training Accuracy: 0.3276\n",
            "  Validation Loss: 1.0987\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 4:\n",
            "  Training Loss: 1.1006\n",
            "  Training Accuracy: 0.3223\n",
            "  Validation Loss: 1.0988\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 5:\n",
            "  Training Loss: 1.0997\n",
            "  Training Accuracy: 0.3338\n",
            "  Validation Loss: 1.0987\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "\n",
            "--- Training History for Adagrad ---\n",
            "Epoch 1:\n",
            "  Training Loss: 1.2695\n",
            "  Training Accuracy: 0.3783\n",
            "  Validation Loss: 1.0430\n",
            "  Validation Accuracy: 0.3929\n",
            "----------------------------------------\n",
            "Epoch 2:\n",
            "  Training Loss: 1.0439\n",
            "  Training Accuracy: 0.4665\n",
            "  Validation Loss: 0.9892\n",
            "  Validation Accuracy: 0.5476\n",
            "----------------------------------------\n",
            "Epoch 3:\n",
            "  Training Loss: 0.9731\n",
            "  Training Accuracy: 0.5185\n",
            "  Validation Loss: 0.9577\n",
            "  Validation Accuracy: 0.4087\n",
            "----------------------------------------\n",
            "Epoch 4:\n",
            "  Training Loss: 0.9217\n",
            "  Training Accuracy: 0.5661\n",
            "  Validation Loss: 0.7488\n",
            "  Validation Accuracy: 0.8254\n",
            "----------------------------------------\n",
            "Epoch 5:\n",
            "  Training Loss: 0.8330\n",
            "  Training Accuracy: 0.6182\n",
            "  Validation Loss: 0.6771\n",
            "  Validation Accuracy: 0.8413\n",
            "----------------------------------------\n",
            "\n",
            "--- Training History for RMSProp ---\n",
            "Epoch 1:\n",
            "  Training Loss: 14.5558\n",
            "  Training Accuracy: 0.3267\n",
            "  Validation Loss: 1.0986\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 2:\n",
            "  Training Loss: 1.1546\n",
            "  Training Accuracy: 0.3201\n",
            "  Validation Loss: 1.0986\n",
            "  Validation Accuracy: 0.3254\n",
            "----------------------------------------\n",
            "Epoch 3:\n",
            "  Training Loss: 1.0989\n",
            "  Training Accuracy: 0.3082\n",
            "  Validation Loss: 1.0986\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 4:\n",
            "  Training Loss: 1.1246\n",
            "  Training Accuracy: 0.3210\n",
            "  Validation Loss: 1.0986\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "Epoch 5:\n",
            "  Training Loss: 1.0987\n",
            "  Training Accuracy: 0.3276\n",
            "  Validation Loss: 1.0986\n",
            "  Validation Accuracy: 0.3333\n",
            "----------------------------------------\n",
            "\n",
            "--- Training History for Adam ---\n",
            "Epoch 1:\n",
            "  Training Loss: 10.8265\n",
            "  Training Accuracy: 0.3563\n",
            "  Validation Loss: 1.0882\n",
            "  Validation Accuracy: 0.3571\n",
            "----------------------------------------\n",
            "Epoch 2:\n",
            "  Training Loss: 1.0538\n",
            "  Training Accuracy: 0.4198\n",
            "  Validation Loss: 1.0125\n",
            "  Validation Accuracy: 0.4246\n",
            "----------------------------------------\n",
            "Epoch 3:\n",
            "  Training Loss: 1.0353\n",
            "  Training Accuracy: 0.4312\n",
            "  Validation Loss: 1.0610\n",
            "  Validation Accuracy: 0.3929\n",
            "----------------------------------------\n",
            "Epoch 4:\n",
            "  Training Loss: 1.0683\n",
            "  Training Accuracy: 0.3801\n",
            "  Validation Loss: 1.0325\n",
            "  Validation Accuracy: 0.4206\n",
            "----------------------------------------\n",
            "Epoch 5:\n",
            "  Training Loss: 1.0215\n",
            "  Training Accuracy: 0.4255\n",
            "  Validation Loss: 1.0868\n",
            "  Validation Accuracy: 0.3651\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_model(optimizer):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(100, 100, 3)),  # Changed input shape to (100, 100, 3)\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),  # <- Added hidden layer\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "b1KzR_BctHlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine optimizer list in case it was lost\n",
        "optimizer_list = {\n",
        "    \"SGD\": tf.keras.optimizers.SGD(),\n",
        "    \"SGD_Momentum\": tf.keras.optimizers.SGD(momentum=0.9),\n",
        "    \"Adagrad\": tf.keras.optimizers.Adagrad(),\n",
        "    \"RMSProp\": tf.keras.optimizers.RMSprop(),\n",
        "    \"Adam\": tf.keras.optimizers.Adam()\n",
        "}\n"
      ],
      "metadata": {
        "id": "3d6GVNaft0d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store training history for CNN models\n",
        "history_cnn = {}\n",
        "best_cnn_model = None\n",
        "best_cnn_optimizer = \"\"\n",
        "best_cnn_acc = 0\n",
        "\n",
        "# Assuming you want to use 10% of the training data for validation\n",
        "val_split = 0.1\n",
        "val_size = int(val_split * len(train_ds_raw))\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "val_ds_raw = train_ds_raw.take(val_size)\n",
        "train_ds_raw = train_ds_raw.skip(val_size)\n",
        "\n",
        "\n",
        "# Apply preprocessing and batching to the validation dataset\n",
        "val_ds = (\n",
        "    val_ds_raw\n",
        "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Train CNN model with each optimizer\n",
        "for name, opt in optimizer_list.items():\n",
        "    print(f\"\\nğŸ”§ Training CNN model with {name} optimizer...\")\n",
        "    model_cnn = cnn_model(opt)  # Create the CNN model using the current optimizer\n",
        "\n",
        "    # Train the model using the correct dataset variables: `train_ds` and `val_ds`\n",
        "    hist = model_cnn.fit(train_ds, validation_data=val_ds, epochs=5, verbose=1)\n",
        "\n",
        "    # Store the history of training and validation metrics\n",
        "    history_cnn[name] = hist.history\n",
        "\n",
        "    # Select best model based on validation accuracy\n",
        "    current_val_acc = max(hist.history['val_accuracy'])\n",
        "    if best_cnn_model is None or current_val_acc > best_cnn_acc:\n",
        "        best_cnn_model = model_cnn\n",
        "        best_cnn_acc = current_val_acc\n",
        "        best_cnn_optimizer = name\n",
        "\n",
        "# Output the best optimizer and model\n",
        "print(f\"\\nâœ… Best CNN optimizer: {best_cnn_optimizer} with Validation Accuracy: {best_cnn_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6WKan2Ut3ll",
        "outputId": "3290c5c6-7989-4b09-c7d8-bcee6d23e93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”§ Training CNN model with SGD optimizer...\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 549ms/step - accuracy: 0.4178 - loss: 1.0779 - val_accuracy: 0.4524 - val_loss: 1.0280\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 533ms/step - accuracy: 0.5592 - loss: 0.9713 - val_accuracy: 0.7579 - val_loss: 0.7669\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 541ms/step - accuracy: 0.6662 - loss: 0.7657 - val_accuracy: 0.7183 - val_loss: 0.6231\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 534ms/step - accuracy: 0.8175 - loss: 0.5095 - val_accuracy: 0.9444 - val_loss: 0.2248\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 592ms/step - accuracy: 0.9376 - loss: 0.2260 - val_accuracy: 0.9643 - val_loss: 0.1358\n",
            "\n",
            "ğŸ”§ Training CNN model with SGD_Momentum optimizer...\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 555ms/step - accuracy: 0.4098 - loss: 1.0672 - val_accuracy: 0.5119 - val_loss: 0.9289\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 545ms/step - accuracy: 0.7593 - loss: 0.6011 - val_accuracy: 0.9881 - val_loss: 0.0592\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 553ms/step - accuracy: 0.9875 - loss: 0.0468 - val_accuracy: 0.9921 - val_loss: 0.0177\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 554ms/step - accuracy: 0.9969 - loss: 0.0114 - val_accuracy: 0.9960 - val_loss: 0.0097\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 554ms/step - accuracy: 0.9983 - loss: 0.0049 - val_accuracy: 0.9960 - val_loss: 0.0038\n",
            "\n",
            "ğŸ”§ Training CNN model with Adagrad optimizer...\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 559ms/step - accuracy: 0.3808 - loss: 1.0940 - val_accuracy: 0.4643 - val_loss: 1.0788\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 565ms/step - accuracy: 0.4651 - loss: 1.0786 - val_accuracy: 0.6190 - val_loss: 1.0655\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 550ms/step - accuracy: 0.5515 - loss: 1.0626 - val_accuracy: 0.7619 - val_loss: 1.0449\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 544ms/step - accuracy: 0.6199 - loss: 1.0319 - val_accuracy: 0.7103 - val_loss: 1.0080\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 549ms/step - accuracy: 0.6292 - loss: 0.9925 - val_accuracy: 0.5873 - val_loss: 0.9593\n",
            "\n",
            "ğŸ”§ Training CNN model with RMSProp optimizer...\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 563ms/step - accuracy: 0.5097 - loss: 1.0266 - val_accuracy: 0.9643 - val_loss: 0.1287\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 561ms/step - accuracy: 0.9681 - loss: 0.0974 - val_accuracy: 0.9960 - val_loss: 0.0162\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 560ms/step - accuracy: 0.9948 - loss: 0.0205 - val_accuracy: 1.0000 - val_loss: 0.0100\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 554ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 4.4252e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 637ms/step - accuracy: 0.9919 - loss: 0.0296 - val_accuracy: 1.0000 - val_loss: 8.3445e-04\n",
            "\n",
            "ğŸ”§ Training CNN model with Adam optimizer...\n",
            "Epoch 1/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 574ms/step - accuracy: 0.6734 - loss: 0.6940 - val_accuracy: 0.9960 - val_loss: 0.0185\n",
            "Epoch 2/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 564ms/step - accuracy: 0.9956 - loss: 0.0174 - val_accuracy: 1.0000 - val_loss: 0.0025\n",
            "Epoch 3/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 566ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 2.1252e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 555ms/step - accuracy: 1.0000 - loss: 1.1413e-04 - val_accuracy: 1.0000 - val_loss: 1.0156e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 551ms/step - accuracy: 1.0000 - loss: 6.9548e-05 - val_accuracy: 1.0000 - val_loss: 9.3095e-05\n",
            "\n",
            "âœ… Best CNN optimizer: RMSProp with Validation Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nğŸ“Š Training History for Each CNN Optimizer:\\n\")\n",
        "\n",
        "for optimizer_name, history in history_cnn.items():\n",
        "    print(f\"ğŸ”§ Optimizer: {optimizer_name}\")\n",
        "    for epoch in range(len(history['loss'])):\n",
        "        train_loss = history['loss'][epoch]\n",
        "        train_acc = history['accuracy'][epoch]\n",
        "        val_loss = history['val_loss'][epoch]\n",
        "        val_acc = history['val_accuracy'][epoch]\n",
        "        print(\n",
        "            f\"  Epoch {epoch+1}: \"\n",
        "            f\"Train Loss = {train_loss:.4f}, \"\n",
        "            f\"Train Acc = {train_acc:.4f}, \"\n",
        "            f\"Val Loss = {val_loss:.4f}, \"\n",
        "            f\"Val Acc = {val_acc:.4f}\"\n",
        "        )\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2hsnq8Q042o",
        "outputId": "23d404d4-bbeb-4afb-e30d-c169a5cef068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Training History for Each CNN Optimizer:\n",
            "\n",
            "ğŸ”§ Optimizer: SGD\n",
            "  Epoch 1: Train Loss = 1.0584, Train Acc = 0.4612, Val Loss = 1.0280, Val Acc = 0.4524\n",
            "  Epoch 2: Train Loss = 0.9294, Train Acc = 0.5785, Val Loss = 0.7669, Val Acc = 0.7579\n",
            "  Epoch 3: Train Loss = 0.7034, Train Acc = 0.7094, Val Loss = 0.6231, Val Acc = 0.7183\n",
            "  Epoch 4: Train Loss = 0.4273, Train Acc = 0.8607, Val Loss = 0.2248, Val Acc = 0.9444\n",
            "  Epoch 5: Train Loss = 0.1980, Train Acc = 0.9458, Val Loss = 0.1358, Val Acc = 0.9643\n",
            "------------------------------------------------------------\n",
            "ğŸ”§ Optimizer: SGD_Momentum\n",
            "  Epoch 1: Train Loss = 1.0408, Train Acc = 0.4581, Val Loss = 0.9289, Val Acc = 0.5119\n",
            "  Epoch 2: Train Loss = 0.3803, Train Acc = 0.8558, Val Loss = 0.0592, Val Acc = 0.9881\n",
            "  Epoch 3: Train Loss = 0.0463, Train Acc = 0.9868, Val Loss = 0.0177, Val Acc = 0.9921\n",
            "  Epoch 4: Train Loss = 0.0097, Train Acc = 0.9974, Val Loss = 0.0097, Val Acc = 0.9960\n",
            "  Epoch 5: Train Loss = 0.0024, Train Acc = 0.9996, Val Loss = 0.0038, Val Acc = 0.9960\n",
            "------------------------------------------------------------\n",
            "ğŸ”§ Optimizer: Adagrad\n",
            "  Epoch 1: Train Loss = 1.0905, Train Acc = 0.4272, Val Loss = 1.0788, Val Acc = 0.4643\n",
            "  Epoch 2: Train Loss = 1.0750, Train Acc = 0.5198, Val Loss = 1.0655, Val Acc = 0.6190\n",
            "  Epoch 3: Train Loss = 1.0561, Train Acc = 0.5736, Val Loss = 1.0449, Val Acc = 0.7619\n",
            "  Epoch 4: Train Loss = 1.0253, Train Acc = 0.6287, Val Loss = 1.0080, Val Acc = 0.7103\n",
            "  Epoch 5: Train Loss = 0.9775, Train Acc = 0.6380, Val Loss = 0.9593, Val Acc = 0.5873\n",
            "------------------------------------------------------------\n",
            "ğŸ”§ Optimizer: RMSProp\n",
            "  Epoch 1: Train Loss = 0.7093, Train Acc = 0.6936, Val Loss = 0.1287, Val Acc = 0.9643\n",
            "  Epoch 2: Train Loss = 0.0873, Train Acc = 0.9744, Val Loss = 0.0162, Val Acc = 0.9960\n",
            "  Epoch 3: Train Loss = 0.0519, Train Acc = 0.9872, Val Loss = 0.0100, Val Acc = 1.0000\n",
            "  Epoch 4: Train Loss = 0.0024, Train Acc = 1.0000, Val Loss = 0.0004, Val Acc = 1.0000\n",
            "  Epoch 5: Train Loss = 0.0310, Train Acc = 0.9916, Val Loss = 0.0008, Val Acc = 1.0000\n",
            "------------------------------------------------------------\n",
            "ğŸ”§ Optimizer: Adam\n",
            "  Epoch 1: Train Loss = 0.3621, Train Acc = 0.8470, Val Loss = 0.0185, Val Acc = 0.9960\n",
            "  Epoch 2: Train Loss = 0.0169, Train Acc = 0.9960, Val Loss = 0.0025, Val Acc = 1.0000\n",
            "  Epoch 3: Train Loss = 0.0008, Train Acc = 1.0000, Val Loss = 0.0002, Val Acc = 1.0000\n",
            "  Epoch 4: Train Loss = 0.0001, Train Acc = 1.0000, Val Loss = 0.0001, Val Acc = 1.0000\n",
            "  Epoch 5: Train Loss = 0.0001, Train Acc = 1.0000, Val Loss = 0.0001, Val Acc = 1.0000\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Function to extract final epoch metrics\n",
        "def extract_final_metrics(history, is_cnn=False):\n",
        "    loss_key = 'loss'\n",
        "    acc_key = 'accuracy' if is_cnn else 'acc'\n",
        "    val_loss_key = 'val_loss'\n",
        "    val_acc_key = 'val_accuracy' if is_cnn else 'val_acc'\n",
        "\n",
        "    results = []\n",
        "    for opt_name, hist in history.items():\n",
        "        results.append([\n",
        "            opt_name,\n",
        "            hist[loss_key][-1],\n",
        "            hist[acc_key][-1],\n",
        "            hist[val_loss_key][-1],\n",
        "            hist[val_acc_key][-1]\n",
        "        ])\n",
        "    return results\n",
        "\n",
        "# Extract metrics for MLP and CNN\n",
        "mlp_metrics = extract_final_metrics(history_mlp, is_cnn=False)\n",
        "cnn_metrics = extract_final_metrics(history_cnn, is_cnn=True)\n",
        "\n",
        "# Add model names for the table\n",
        "mlp_rows = [[\"MLP\", *row] for row in mlp_metrics]\n",
        "cnn_rows = [[\"CNN\", *row] for row in cnn_metrics]\n",
        "\n",
        "# Combine for a full table\n",
        "full_results = mlp_rows + cnn_rows\n",
        "\n",
        "# Define headers\n",
        "headers = [\"Model\", \"Optimizer\", \"Train Loss\", \"Train Acc\", \"Validation Loss\", \"Validation Acc\"]\n",
        "\n",
        "# Print table using tabulate\n",
        "print(tabulate(full_results, headers=headers, floatfmt=\".4f\", tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5lIqM90INyU",
        "outputId": "278077ce-10c4-4b72-b827-e25be4c55e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| Model   | Optimizer    |   Train Loss |   Train Acc |   Validation Loss |   Validation Acc |\n",
            "+=========+==============+==============+=============+===================+==================+\n",
            "| MLP     | SGD          |       1.0990 |      0.3316 |            1.0987 |           0.3333 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| MLP     | SGD_Momentum |       1.0997 |      0.3338 |            1.0987 |           0.3333 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| MLP     | Adagrad      |       0.8330 |      0.6182 |            0.6771 |           0.8413 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| MLP     | RMSProp      |       1.0987 |      0.3276 |            1.0986 |           0.3333 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| MLP     | Adam         |       1.0215 |      0.4255 |            1.0868 |           0.3651 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| CNN     | SGD          |       0.1980 |      0.9458 |            0.1358 |           0.9643 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| CNN     | SGD_Momentum |       0.0024 |      0.9996 |            0.0038 |           0.9960 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| CNN     | Adagrad      |       0.9775 |      0.6380 |            0.9593 |           0.5873 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| CNN     | RMSProp      |       0.0310 |      0.9916 |            0.0008 |           1.0000 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n",
            "| CNN     | Adam         |       0.0001 |      1.0000 |            0.0001 |           1.0000 |\n",
            "+---------+--------------+--------------+-------------+-------------------+------------------+\n"
          ]
        }
      ]
    }
  ]
}